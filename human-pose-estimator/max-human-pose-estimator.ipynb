{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Code Model Asset Exchange Human Pose Estimator\n",
    "\n",
    "https://github.com/IBM/MAX-Human-Pose-Estimator\n",
    "\n",
    "notes/links/etc\n",
    "\n",
    "- `Netron`: https://github.com/lutzroeder/Netron\n",
    "- `TF.js Converter`: https://github.com/tensorflow/tfjs-converter\n",
    "- `TF.js API`: https://js.tensorflow.org/api/latest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. In a terminal window, run the following commands to download and extract the model artifacts for the Human Pose Estimator:\n",
    "    ```\n",
    "    curl -O http://max-assets.s3-api.us-geo.objectstorage.softlayer.net/human-pose-estimator/1.0/assets.tar.gz\n",
    "    \n",
    "    tar -zxvf assets.tar.gz\n",
    "    ```    \n",
    "\n",
    "2. Run the notebook `jupyter notebook .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been tested with Python version 3.6.6\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been tested with tensorflow 1.12.0, tensorflowjs 0.8.0, numpy 1.16.1, opencv-python 3.4.4.19\n",
    "!pip show tensorflow tensorflowjs numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the packages needed\n",
    "\n",
    "# !pip install -Iv tensorflow\n",
    "# !pip install -Iv tensorflowjs\n",
    "# !pip install -Iv numpy\n",
    "# !pip install -Iv opencv-python\n",
    "\n",
    "# Restart the kernel after installation completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<strong>NOTE</strong>:\n",
    "\n",
    "The post-processing for Part-Affinity Fields Map is implemented in C++ & Swig:\n",
    "\n",
    "https://github.com/ildoonet/tf-pose-estimation/tree/master/tf_pose/pafprocess\n",
    "\n",
    "To run locally on your machine:\n",
    "\n",
    "- First install `swig`. For Mac OS X, you can install `swig` via `homebrew`\n",
    "\n",
    "```\n",
    "brew install swig\n",
    "```\n",
    "\n",
    "- Then build the `pafprocess` module:\n",
    "\n",
    "```\n",
    "swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Import libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import io\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pprint\n",
    "import cv2\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Update the variable with the appropriate directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full path to cloned repo\n",
    "human_pose_estimator = '/Users/va/machine-learning/MAX-Human-Pose-Estimator'\n",
    "\n",
    "# full path to extracted frozen graph\n",
    "frozen_graph_path = '/Users/va/machine-learning/human-pose-estimator-model/human-pose-estimator-tensorflow.pb'\n",
    "\n",
    "# Add the repo to the Python path\n",
    "sys.path.append(human_pose_estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TF versions:', tf.GIT_VERSION, tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Set the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "config.DEFAULT_MODEL_PATH = frozen_graph_path\n",
    "\n",
    "\n",
    "def read_image(image_data):\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    image = np.array(image)[:,:,::-1] # Convert RGB to BGR for OpenCV\n",
    "    return image\n",
    "\n",
    "\n",
    "# Define the colors for different human body parts.\n",
    "CocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], \n",
    "              [170, 255, 0], [85, 255, 0], [0, 255, 0], [0, 255, 85], \n",
    "              [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], \n",
    "              [0, 0, 255], [85, 0, 255], [170, 0, 255], [255, 0, 255], \n",
    "              [255, 0, 170], [255, 0, 85]]\n",
    "\n",
    "\n",
    "# Visualize the detected human poses on the image. The returned JSON result \n",
    "# contains the pose lines for each person in the input image. Each \n",
    "# person may have multiple pose lines. Each pose line contains four coordinates \n",
    "# for the start and end points as [x1, y1, x2, y2]. The `cv2.line(img, (x1, y1), \n",
    "# (x2, y2), color, thickness)` function is utilized to visualize the detected\n",
    "# pose lines.\n",
    "def draw_pose(humans, img):\n",
    "    for human in humans:\n",
    "        pose_lines = human['pose_lines']\n",
    "        for i in range(len(pose_lines)):\n",
    "            line = pose_lines[i]['line']\n",
    "            cv2.line(img, (line[0], line[1]), (line[2], line[3]), CocoColors[i], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core\n",
    "from core.tf_pose.estimator import TfPoseEstimator\n",
    "\n",
    "test_image_path = '/Users/va/machine-learning/images/img-03.jpg'\n",
    "\n",
    "model = TfPoseEstimator(frozen_graph_path, target_size=config.DEFAULT_IMAGE_SIZE)\n",
    "\n",
    "# run prediction on image\n",
    "with open(test_image_path, 'rb') as image:\n",
    "    img_array = read_image(image.read())\n",
    "    humans = model.inference(img_array, resize_to_default=True, upsample_size=4.0)\n",
    "    results = TfPoseEstimator.draw_human_pose_connection(img_array, humans)\n",
    "    pp.pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Visualize the detected poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the detected poses overlaid on the original image.\n",
    "\n",
    "org_img = cv2.imread(test_image_path)[:,:,::-1]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(org_img)\n",
    "plt.title(\"The original image\")\n",
    "\n",
    "pose_img = np.zeros(org_img.shape, dtype=np.uint8)\n",
    "draw_pose(results, pose_img)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(pose_img)\n",
    "plt.title(\"The detected poses\")\n",
    "\n",
    "overlaid_img = org_img.copy()\n",
    "draw_pose(results, overlaid_img)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(overlaid_img)\n",
    "plt.title(\"Poses overlaid on original image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Converting to a web-friendly format\n",
    "\n",
    "[https://github.com/tensorflow/tfjs-converter](https://github.com/tensorflow/tfjs-converter)\n",
    "\n",
    "\n",
    "```\n",
    "tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names='Openpose/concat_stage7' \\\n",
    "    /path/to/frozen/model.pb \\\n",
    "    /path/to/web_asset_output_dir\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set appropriate desired output path for web format\n",
    "web_asset_dir = '/Users/va/machine-learning/web-assets/human-pose-estimator'\n",
    "\n",
    "# create directory if it does not exist\n",
    "pathlib.Path(web_asset_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names='Openpose/concat_stage7' \\\n",
    "    {frozen_graph_path} \\\n",
    "    {web_asset_dir}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Web asset directory {}:\".format(web_asset_dir))\n",
    "\n",
    "web_assets = os.listdir(web_asset_dir)\n",
    "web_assets.sort()\n",
    "\n",
    "for file in web_assets:\n",
    "    file_stat = os.stat(\"{}/{}\".format(web_asset_dir,file))\n",
    "    print(\" {} {} {:>20}\".format(file.ljust(30), time.ctime(file_stat.st_mtime), file_stat.st_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
