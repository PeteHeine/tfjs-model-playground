{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Code Model Asset Exchange Human Pose Estimator\n",
    "\n",
    "https://github.com/IBM/MAX-Human-Pose-Estimator\n",
    "\n",
    "notes/links/etc\n",
    "\n",
    "- `Netron`: https://github.com/lutzroeder/Netron\n",
    "- `TF.js Converter`: https://github.com/tensorflow/tfjs-converter\n",
    "- `TF.js API`: https://js.tensorflow.org/api/latest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Run inference using the repo](#Run-inference-using-the-repo)\n",
    "1. [Print the graph nodes](#Print-the-graph-nodes)\n",
    "1. [Convert the model to a web-friendly format](#Convert-the-model-to-a-web-friendly-format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Clone the MAX Human Pose Estimator GitHub repository:\n",
    "\n",
    "    ```\n",
    "    git clone https://github.com/IBM/MAX-Human-Pose-Estimator.git\n",
    "    ```\n",
    "    \n",
    "\n",
    "- Download and extract the model artifacts for the Human Pose Estimator:\n",
    "\n",
    "    [http://max-assets.s3-api.us-geo.objectstorage.softlayer.net/human-pose-estimator/1.0/assets.tar.gz](http://max-assets.s3-api.us-geo.objectstorage.softlayer.net/human-pose-estimator/1.0/assets.tar.gz)\n",
    "\n",
    "    For example, from a terminal window:\n",
    "\n",
    "    ```\n",
    "    curl -O http://max-assets.s3-api.us-geo.objectstorage.softlayer.net/human-pose-estimator/1.0/assets.tar.gz\n",
    "\n",
    "    tar -zxvf assets.tar.gz\n",
    "    ```\n",
    "\n",
    "- If you want to run the MAX Human Pose Estimator locally on your machine, you will need to build the `pafprocess`:\n",
    "\n",
    "    1. Install `swig`. For Mac OS X, you can install `swig` via `homebrew`\n",
    "    \n",
    "    ```\n",
    "    brew install swig\n",
    "    ```\n",
    "    \n",
    "    1. From a terminal window, go to the `/pafprocess` in the repo:\n",
    "    \n",
    "    ```\n",
    "    cd <human-pose-estimator-repo>/core/tf_pose/pafprocess\n",
    "    ```\n",
    "    \n",
    "    1. Run the command:\n",
    "    \n",
    "    ```\n",
    "    swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n",
    "    ```\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been tested with Python version 3.6.6\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been tested with tensorflow 1.12.0, tensorflowjs 0.8.0, and numpy 1.15.1, opencv-python 3.4.4.19\n",
    "!pip show tensorflow tensorflowjs numpy opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the packages needed\n",
    "\n",
    "# !pip install -Iv tensorflow\n",
    "# !pip install -Iv tensorflowjs\n",
    "# !pip install -Iv numpy\n",
    "# !pip install -Iv opencv-python\n",
    "\n",
    "# Restart the kernel after installation completes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<strong>NOTE</strong>: Update the variables with the appropriate directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full path to cloned repo\n",
    "human_pose_estimator = '/Users/va/Desktop/max/repos/MAX-Human-Pose-Estimator'\n",
    "\n",
    "# full path to extracted frozen graph\n",
    "frozen_graph_path = '/Users/va/Desktop/max/human-pose-estimator/model/human-pose-estimator-tensorflow.pb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Run inference using the repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# add the repo to the Python path\n",
    "sys.path.append(human_pose_estimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import core\n",
    "from core.tf_pose.estimator import TfPoseEstimator\n",
    "\n",
    "model = TfPoseEstimator(frozen_graph_path, target_size=config.DEFAULT_IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<strong>NOTE</strong>: Update the variables with the appropriate path to an image to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full path to a test image\n",
    "test_image_path = '/Users/va/Desktop/max/test/img-03.jpg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Run prediction on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def read_image(image_data):\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    image = np.array(image)[:,:,::-1] # Convert RGB to BGR for OpenCV\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction on an image\n",
    "with open(test_image_path, 'rb') as image:\n",
    "    img_array = read_image(image.read())\n",
    "    humans = model.inference(img_array, resize_to_default=True, upsample_size=4.0)\n",
    "    results = TfPoseEstimator.draw_human_pose_connection(img_array, humans)\n",
    "    pp.pprint(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Visualize the detected poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define the colors for different human body parts.\n",
    "CocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], \n",
    "              [170, 255, 0], [85, 255, 0], [0, 255, 0], [0, 255, 85], \n",
    "              [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], \n",
    "              [0, 0, 255], [85, 0, 255], [170, 0, 255], [255, 0, 255], \n",
    "              [255, 0, 170], [255, 0, 85]]\n",
    "\n",
    "\n",
    "# Each person may have multiple pose lines. Each pose line contains four coordinates \n",
    "# for the start and end points as [x1, y1, x2, y2]. The `cv2.line(img, (x1, y1), \n",
    "# (x2, y2), color, thickness)` function is utilized to visualize the detected\n",
    "# pose lines.\n",
    "def draw_pose(humans, img):\n",
    "    for human in humans:\n",
    "        pose_lines = human['pose_lines']\n",
    "        for i in range(len(pose_lines)):\n",
    "            line = pose_lines[i]['line']\n",
    "            cv2.line(img, (line[0], line[1]), (line[2], line[3]), CocoColors[i], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the detected poses overlaid on the original image.\n",
    "\n",
    "org_img = cv2.imread(test_image_path)[:,:,::-1]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(org_img)\n",
    "plt.title(\"The original image\")\n",
    "\n",
    "pose_img = np.zeros(org_img.shape, dtype=np.uint8)\n",
    "draw_pose(results, pose_img)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(pose_img)\n",
    "plt.title(\"The detected poses\")\n",
    "\n",
    "overlaid_img = org_img.copy()\n",
    "draw_pose(results, overlaid_img)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(overlaid_img)\n",
    "plt.title(\"Poses overlaid on original image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Print the graph nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('TF versions:', tf.GIT_VERSION, tf.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Load the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the frozen file and parse it to get the unserialized graph_def\n",
    "def load_frozen_graph(graph_path):\n",
    "    with tf.gfile.GFile(graph_path, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        return graph_def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "restored_graph_def = load_frozen_graph(frozen_graph_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Print graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print list graph nodes/tensors\n",
    "def list_nodes(graph_def):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def,\n",
    "            input_map=None,\n",
    "            return_elements=None,\n",
    "            name=\"\"\n",
    "        )\n",
    "\n",
    "    sess = tf.Session(graph=graph)\n",
    "    nodes = sess.graph.as_graph_def().node\n",
    "    print('graph has {} nodes \\r\\n'.format(len(nodes)))\n",
    "    \n",
    "    for n in nodes:\n",
    "        print(n.name + '=>' +  n.op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list graph nodes\n",
    "list_nodes(restored_graph_def)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Convert the model to a web-friendly format\n",
    "\n",
    "[https://github.com/tensorflow/tfjs-converter](https://github.com/tensorflow/tfjs-converter)\n",
    "\n",
    "\n",
    "```\n",
    "tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names='Openpose/concat_stage7' \\\n",
    "    /path/to/frozen/model.pb \\\n",
    "    /path/to/web_asset_output_dir\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for tensorflowjs_converter\n",
    "!tensorflowjs_converter --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<strong>NOTE</strong>: Update the variables with the appropriate graph output nodes and path to save the converted model assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the graph output nodes\n",
    "graph_output_nodes = 'Openpose/concat_stage7'\n",
    "\n",
    "# set appropriate desired output path for web format\n",
    "web_asset_dir = '/Users/va/Desktop/max/human-pose-estimator/model-tfjs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# create directory if it does not exist\n",
    "pathlib.Path(web_asset_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Run the converter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names={graph_output_nodes} \\\n",
    "    {frozen_graph_path} \\\n",
    "    {web_asset_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Print the converted model assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Web asset directory {}:\".format(web_asset_dir))\n",
    "\n",
    "web_assets = os.listdir(web_asset_dir)\n",
    "web_assets.sort()\n",
    "\n",
    "for file in web_assets:\n",
    "    file_stat = os.stat(\"{}/{}\".format(web_asset_dir,file))\n",
    "    print(\" {} {} {:>20}\".format(file.ljust(30), time.ctime(file_stat.st_mtime), file_stat.st_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
